import * as vscode from 'vscode';
import { Logger } from '../utils/logger';
import { SessionContext } from '../types/session';
import { toolExecutor, getAvailableTools } from './toolExecutor';
import { generateToolInventoryText } from '../tools/index';
import { InputAnalyzer, InputAnalysisResult } from '../utils/inputAnalyzer';

/**
 * Orchestrator v2.0 - AIæ™ºèƒ½å·¥å…·ä»£ç†æ¨¡å¼
 * 
 * æ ¸å¿ƒè®¾è®¡ç†å¿µï¼š
 * ğŸ¤– AIåŠ©æ‰‹æ¨¡å¼ï¼šAIä¸å†æ˜¯æ•°æ®åŒæ­¥çš„è¾…åŠ©å·¥å…·ï¼Œè€Œæ˜¯ç”¨æˆ·çš„æ™ºèƒ½åŠ©æ‰‹
 * ğŸ› ï¸ å·¥å…·ä»£ç†æ¨¡å¼ï¼šAIé€šè¿‡å·¥å…·ä»£ç†å®ç°å„ç§å¤æ‚æ“ä½œ
 * ğŸ¯ æ„å›¾ç†è§£ï¼šAIç†è§£ç”¨æˆ·æ„å›¾ï¼Œè‡ªåŠ¨è§„åˆ’å’Œæ‰§è¡Œå·¥å…·è°ƒç”¨
 * ğŸ”„ åŠ¨æ€é€‚åº”ï¼šæ ¹æ®æ‰§è¡Œç»“æœåŠ¨æ€è°ƒæ•´åç»­æ­¥éª¤
 */

export class Orchestrator {
    private logger = Logger.getInstance();
    private useAIOrchestrator: boolean = true;
    
    constructor() {
        this.loadConfiguration();
        this.logger.info('ğŸ¯ Orchestrator v2.0 initialized - AIæ™ºèƒ½å·¥å…·ä»£ç†æ¨¡å¼');
    }

    /**
     * å¤„ç†ç”¨æˆ·è¾“å…¥ - v2.1ç‰ˆæœ¬ï¼ˆæ”¯æŒå¯¹è¯å¼è§„åˆ’å¾ªç¯çš„æ™ºèƒ½å·¥å…·ä»£ç†æ¨¡å¼ï¼‰
     */
    public async processUserInput(
        userInput: string, 
        sessionContext: SessionContext,
        selectedModel: vscode.LanguageModelChat
    ): Promise<{ intent: string; result?: any }> {
        try {
            this.logger.info(`ğŸš€ Processing with Chain-of-Thought Tool Agent Mode: ${selectedModel.name}`);
            
            // æ£€æŸ¥å®‰å…¨æƒé™
            if (!await this.checkModelPermissions(selectedModel)) {
                throw new Error('No permission to use selected language model');
            }

            // ğŸš€ å¯¹è¯å¼è§„åˆ’å¾ªç¯ï¼šæ”¯æŒè‡ªæˆ‘ä¿®æ­£å’ŒåŠ¨æ€è°ƒæ•´
            return await this.executeConversationalPlanning(userInput, sessionContext, selectedModel);

        } catch (error) {
            this.logger.error('Chain-of-Thought tool agent processing failed', error as Error);
            
            // é™çº§åˆ°ç®€å•å“åº”
            return { 
                intent: 'error', 
                result: `å¤„ç†å¤±è´¥: ${(error as Error).message}` 
            };
        }
    }

    /**
     * ğŸš€ å¯¹è¯å¼è§„åˆ’å¾ªç¯æ‰§è¡Œå™¨ï¼šå®ç°æ€ç»´é“¾ã€è‡ªæˆ‘ä¿®æ­£å’Œæ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†
     */
    private async executeConversationalPlanning(
        userInput: string,
        sessionContext: SessionContext,
        selectedModel: vscode.LanguageModelChat
    ): Promise<{ intent: string; result?: any }> {
        const conversationHistory: Array<{
            role: 'user' | 'ai' | 'system';
            content: string;
            toolResults?: any[];
            tokens?: number;
        }> = [];
        
        const allExecutionResults: any[] = [];
        let totalToolsExecuted = 0;
        const maxIterations = 8; // å¢åŠ åˆ°8è½®ï¼Œæ”¯æŒæ›´å¤æ‚çš„ä»»åŠ¡
        let currentIteration = 0;
        
        // ğŸš€ åŠ¨æ€ä¸Šä¸‹æ–‡ç®¡ç†é…ç½®
        const contextConfig = await this.getContextWindowConfig(selectedModel);
        
        // åˆå§‹ç”¨æˆ·è¾“å…¥
        conversationHistory.push({
            role: 'user',
            content: userInput,
            tokens: this.estimateTokens(userInput)
        });
        
        while (currentIteration < maxIterations) {
            currentIteration++;
            this.logger.info(`ğŸ”„ Chain-of-Thought Iteration ${currentIteration}/${maxIterations}`);
            
            // ğŸš€ æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©å¯¹è¯å†å²
            await this.manageConversationContext(conversationHistory, contextConfig, selectedModel);
            
            // ç”Ÿæˆå½“å‰é˜¶æ®µçš„å·¥å…·è°ƒç”¨è®¡åˆ’
            const toolPlan = await this.generateAdaptiveToolPlan(
                userInput, 
                sessionContext, 
                selectedModel, 
                conversationHistory
            );
            
            if (!toolPlan || toolPlan.length === 0) {
                this.logger.info('ğŸ¯ AI indicates task completion or no further tools needed');
                break;
            }
            
            // ğŸš€ æ£€æŸ¥æ˜¯å¦æœ‰ finalAnswer å·¥å…·è°ƒç”¨
            const finalAnswerCall = toolPlan.find(tool => tool.name === 'finalAnswer');
            if (finalAnswerCall) {
                this.logger.info('ğŸ¯ AI called finalAnswer tool - task completion detected');
                const finalResult = await toolExecutor.executeTool('finalAnswer', finalAnswerCall.args);
                
                return {
                    intent: 'task_completed',
                    result: {
                        mode: 'chain_of_thought_agent_completed',
                        summary: finalResult.result?.summary || 'ä»»åŠ¡å·²å®Œæˆ',
                        finalResult: finalResult.result?.result || '',
                        achievements: finalResult.result?.achievements || [],
                        nextSteps: finalResult.result?.nextSteps,
                        iterations: currentIteration,
                        totalToolsExecuted,
                        conversationHistory: conversationHistory.length,
                        allResults: allExecutionResults
                    }
                };
            }
            
            // æ‰§è¡Œå½“å‰é˜¶æ®µçš„å·¥å…·è°ƒç”¨
            this.logger.info(`ğŸ”§ Executing ${toolPlan.length} tools in iteration ${currentIteration}`);
            const iterationResults = await this.executeToolCalls(toolPlan);
            
            allExecutionResults.push(...iterationResults);
            totalToolsExecuted += toolPlan.length;
            
            // å°†æ‰§è¡Œç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
            const resultsContent = this.formatToolResults(iterationResults);
            conversationHistory.push({
                role: 'system',
                content: resultsContent,
                toolResults: iterationResults,
                tokens: this.estimateTokens(resultsContent)
            });
            
            // æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥çš„å·¥å…·è°ƒç”¨éœ€è¦è‡ªæˆ‘ä¿®æ­£
            const failedTools = iterationResults.filter(r => !r.success);
            if (failedTools.length > 0) {
                this.logger.warn(`âš ï¸ ${failedTools.length} tools failed, will attempt self-correction in next iteration`);
            }
            
            // æ£€æŸ¥æ˜¯å¦è¾¾åˆ°äº†ç”¨æˆ·çš„å®Œæ•´æ„å›¾ï¼ˆåŸºäºæˆåŠŸç‡å’Œå¤æ‚åº¦ï¼‰
            const successRate = iterationResults.filter(r => r.success).length / iterationResults.length;
            if (successRate === 1.0 && this.isSimpleTask(userInput)) {
                this.logger.info('âœ… Simple task completed successfully, ending conversation');
                break;
            }
        }
        
        // å¦‚æœè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œç”Ÿæˆæœ€ç»ˆæ€»ç»“
        const successfulResults = allExecutionResults.filter(r => r.success);
        const failedResults = allExecutionResults.filter(r => !r.success);
        
        const resultSummary = this.summarizeConversationalResults(
            allExecutionResults, 
            currentIteration,
            conversationHistory
        );
        
        return {
            intent: 'conversational_tool_execution',
            result: {
                mode: 'chain_of_thought_agent',
                summary: resultSummary,
                iterations: currentIteration,
                totalToolsExecuted,
                successful: successfulResults.length,
                failed: failedResults.length,
                conversationHistory: conversationHistory.length,
                details: allExecutionResults,
                reachedMaxIterations: currentIteration >= maxIterations
            }
        };
    }

    /**
     * ğŸš€ ç”Ÿæˆè‡ªé€‚åº”å·¥å…·è°ƒç”¨è®¡åˆ’ï¼ˆæ”¯æŒå¯¹è¯å†å² + é”™è¯¯å­¦ä¹ ï¼‰
     */
    private async generateAdaptiveToolPlan(
        userInput: string,
        sessionContext: SessionContext,
        selectedModel: vscode.LanguageModelChat,
        conversationHistory: Array<{ role: string; content: string; toolResults?: any[] }>
    ): Promise<Array<{ name: string; args: any }>> {
        try {
            const adaptivePrompt = await this.buildAdaptiveToolPlanningPrompt(
                userInput, 
                sessionContext, 
                conversationHistory
            );
            
            // ä¼°ç®—æç¤ºè¯çš„tokenæ•°é‡ç”¨äºé”™è¯¯å­¦ä¹ 
            const estimatedTokens = this.estimateTokens(adaptivePrompt);
            
            const messages = [
                vscode.LanguageModelChatMessage.User(adaptivePrompt)
            ];

            const requestOptions: vscode.LanguageModelChatRequestOptions = {
                justification: 'Generate adaptive tool execution plan with conversation history'
            };

            const response = await selectedModel.sendRequest(messages, requestOptions);
            
            let result = '';
            for await (const fragment of response.text) {
                result += fragment;
            }

            return this.parseToolPlanFromResponse(result);
            
        } catch (error) {
            const errorObj = error as Error;
            this.logger.error('Failed to generate adaptive tool plan', errorObj);
            
            // ğŸš€ é”™è¯¯åé¦ˆå­¦ä¹ ï¼šå¦‚æœæ˜¯ä¸Šä¸‹æ–‡é”™è¯¯ï¼Œæ›´æ–°æ¨¡å‹é…ç½®
            const adaptivePrompt = await this.buildAdaptiveToolPlanningPrompt(
                userInput, 
                sessionContext, 
                conversationHistory
            );
            const estimatedTokens = this.estimateTokens(adaptivePrompt);
            await this.handleContextError(errorObj, selectedModel, estimatedTokens);
            
            return [];
        }
    }

    /**
     * ğŸš€ æ„å»ºè‡ªé€‚åº”å·¥å…·è§„åˆ’æç¤ºè¯ï¼ˆåŒ…å«å¯¹è¯å†å²ï¼‰
     */
    private async buildAdaptiveToolPlanningPrompt(
        userInput: string,
        sessionContext: SessionContext,
        conversationHistory: Array<{ role: string; content: string; toolResults?: any[] }>
    ): Promise<string> {
        const availableTools = getAvailableTools();
        const toolsJson = JSON.stringify(availableTools, null, 2);
        
        const hasActiveProject = !!sessionContext.projectName;
        const projectName = sessionContext.projectName || 'none';
        
        // RAGçŸ¥è¯†æ£€ç´¢
        const relevantKnowledge = await this.retrieveRelevantKnowledge(userInput, sessionContext);
        
        // æ„å»ºå¯¹è¯å†å²æ‘˜è¦
        const historyContext = this.buildHistoryContext(conversationHistory);
        
        return `# Role
You are an AI Assistant specialized in Software Requirements Specification (SRS) management.
You have access to a set of tools and can engage in multi-turn conversations to accomplish complex tasks.

# Original User Request
"${userInput}"

# Current Context
- Has active project: ${hasActiveProject}
- Project name: ${projectName}
- Available artifacts: ${JSON.stringify(sessionContext.activeFiles || [])}
${relevantKnowledge ? `\n# Relevant Knowledge & Best Practices\n${relevantKnowledge}\n` : ''}
${historyContext ? `\n# Previous Actions & Results\n${historyContext}\n` : ''}
# Available Tools
${toolsJson}

# Task
Based on the original user request and the conversation history above, determine what tools to execute next.

**Chain-of-Thought Analysis:**
1. What has been accomplished so far?
2. What still needs to be done to fulfill the original user request?
3. Do any previous tool failures need correction?
4. What is the next logical step?

# Output Format
Respond with a JSON object:
{
    "thought": "Your reasoning about what to do next based on the conversation history",
    "tool_calls": [
        {
            "name": "toolName",
            "args": { /* arguments */ }
        }
    ]
}

**Important Instructions:**
- When you have COMPLETELY finished the user's task, call the finalAnswer tool with a comprehensive summary
- If you need to retry a failed tool with different parameters, explain your correction strategy  
- Focus on making progress toward the original user goal
- Consider dependencies between tools and execute them in logical sequence
- The finalAnswer tool is the ONLY way to properly signal task completion

**Task Completion Example:**
{
    "thought": "I have successfully completed all aspects of the user's request...",
    "tool_calls": [
        {
            "name": "finalAnswer",
            "args": {
                "summary": "Complete summary of what was accomplished",
                "result": "Final outcome and status",
                "achievements": ["Specific achievement 1", "Specific achievement 2"],
                "nextSteps": "Optional suggestions for what to do next"
            }
        }
    ]
}

Generate your response now:`;
    }

    /**
     * æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡æ‘˜è¦
     */
    private buildHistoryContext(conversationHistory: Array<{ role: string; content: string; toolResults?: any[] }>): string {
        const historyItems: string[] = [];
        
        conversationHistory.forEach((item, index) => {
            if (item.role === 'system' && item.toolResults) {
                const successCount = item.toolResults.filter(r => r.success).length;
                const totalCount = item.toolResults.length;
                const toolNames = item.toolResults.map(r => r.toolName).join(', ');
                
                historyItems.push(`**Step ${Math.floor(index/2) + 1}**: Executed tools [${toolNames}] - ${successCount}/${totalCount} successful`);
                
                // æ·»åŠ å¤±è´¥å·¥å…·çš„è¯¦ç»†ä¿¡æ¯
                const failures = item.toolResults.filter(r => !r.success);
                if (failures.length > 0) {
                    failures.forEach(failure => {
                        historyItems.push(`  âŒ ${failure.toolName} failed: ${failure.error || 'Unknown error'}`);
                    });
                }
            }
        });
        
        return historyItems.join('\n');
    }

    /**
     * åˆ¤æ–­æ˜¯å¦ä¸ºç®€å•ä»»åŠ¡
     */
    private isSimpleTask(userInput: string): boolean {
        const simpleKeywords = ['åˆ—å‡º', 'æ˜¾ç¤º', 'æŸ¥çœ‹', 'è·å–', 'çŠ¶æ€'];
        return simpleKeywords.some(keyword => userInput.includes(keyword));
    }

    /**
     * æ±‡æ€»å¯¹è¯å¼æ‰§è¡Œç»“æœ
     */
    private summarizeConversationalResults(
        results: any[], 
        iterations: number, 
        conversationHistory: any[]
    ): string {
        const successful = results.filter(r => r.success);
        const failed = results.filter(r => !r.success);
        
        let summary = `âœ… é€šè¿‡ ${iterations} è½®å¯¹è¯å®Œæˆä»»åŠ¡ï¼š`;
        summary += `\n  - æˆåŠŸæ‰§è¡Œï¼š${successful.length} ä¸ªå·¥å…·`;
        
        if (failed.length > 0) {
            summary += `\n  - æ‰§è¡Œå¤±è´¥ï¼š${failed.length} ä¸ªå·¥å…·`;
        }
        
        summary += `\n  - å¯¹è¯è½®æ¬¡ï¼š${iterations}`;
        
        return summary;
    }

    /**
     * ğŸš€ ç”Ÿæˆå·¥å…·è°ƒç”¨è®¡åˆ’ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰
     */
    private async generateToolPlan(
        userInput: string,
        sessionContext: SessionContext,
        selectedModel: vscode.LanguageModelChat
    ): Promise<Array<{ name: string; args: any }>> {
        // ä½¿ç”¨æ–°çš„è‡ªé€‚åº”å·¥å…·è§„åˆ’æ–¹æ³•ï¼Œä½†ä¸åŒ…å«å¯¹è¯å†å²
        return await this.generateAdaptiveToolPlan(userInput, sessionContext, selectedModel, []);
    }

    /**
     * ğŸš€ æ„å»ºå·¥å…·è§„åˆ’æç¤ºè¯ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒRAGçŸ¥è¯†æ£€ç´¢å’Œæ€ç»´é“¾ï¼‰
     */
    private async buildToolPlanningPrompt(userInput: string, sessionContext: SessionContext): Promise<string> {
        const availableTools = getAvailableTools();
        const toolsJson = JSON.stringify(availableTools, null, 2);
        
        const hasActiveProject = !!sessionContext.projectName;
        const projectName = sessionContext.projectName || 'none';
        
        // ğŸš€ RAGå¢å¼ºï¼šåŸºäºç”¨æˆ·è¾“å…¥è¿›è¡ŒçŸ¥è¯†æ£€ç´¢
        const relevantKnowledge = await this.retrieveRelevantKnowledge(userInput, sessionContext);

        return `# Role
You are an AI Assistant specialized in Software Requirements Specification (SRS) management.
You have access to a set of tools to help users manage their SRS projects.

# Context
- User input: "${userInput}"
- Has active project: ${hasActiveProject}
- Project name: ${projectName}
- Available artifacts: ${JSON.stringify(sessionContext.activeFiles || [])}
${relevantKnowledge ? `\n# Relevant Knowledge & Best Practices\n${relevantKnowledge}\n` : ''}
# Available Tools
${toolsJson}

# Task
Analyze the user's request and create a detailed execution plan using the available tools.

**Think step by step (Chain-of-Thought reasoning):**
1. What is the user trying to accomplish?
2. What information do I need to gather first?
3. What are the dependencies between different actions?
4. What is the optimal sequence of tool calls?

# Output Format
Respond with a JSON object that includes your reasoning and the tool execution plan:
{
    "thought": "Step-by-step explanation of your reasoning process and strategy",
    "tool_calls": [
        {
            "name": "toolName",
            "args": {
                // tool-specific arguments based on the tool's parameter schema
            }
        }
    ]
}

**Important**: If you need information from one tool to make decisions about subsequent tools, include only the first tool(s) in the plan. The system will execute them and provide results for further planning.

Generate the tool execution plan now:`;
    }

    /**
     * ğŸš€ è§£æAIè¿”å›çš„å·¥å…·è§„åˆ’ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒæ€ç»´é“¾æ ¼å¼ï¼‰
     */
    private parseToolPlanFromResponse(response: string): Array<{ name: string; args: any }> {
        try {
            // å°è¯•æå–JSONå¯¹è±¡ï¼ˆæ–°æ ¼å¼ï¼ŒåŒ…å«thoughtå’Œtool_callsï¼‰
            const jsonMatch = response.match(/\{[\s\S]*?\}/);
            if (jsonMatch) {
                const parsed = JSON.parse(jsonMatch[0]);
                
                // æ–°æ ¼å¼ï¼šåŒ…å«thoughtå’Œtool_calls
                if (parsed.tool_calls && Array.isArray(parsed.tool_calls)) {
                    this.logger.info(`AIæ€ç»´é“¾: ${parsed.thought || 'No reasoning provided'}`);
                    this.logger.info(`Parsed ${parsed.tool_calls.length} tool calls from AI response`);
                    return parsed.tool_calls;
                }
                
                // å…¼å®¹æ€§ï¼šå¦‚æœæ˜¯ç›´æ¥çš„æ•°ç»„æ ¼å¼
                if (Array.isArray(parsed)) {
                    this.logger.info(`Parsed ${parsed.length} tool calls from AI response (legacy format)`);
                    return parsed;
                }
            }
            
            // å›é€€ï¼šå°è¯•æå–JSONæ•°ç»„ï¼ˆæ—§æ ¼å¼å…¼å®¹ï¼‰
            const arrayMatch = response.match(/\[[\s\S]*?\]/);
            if (arrayMatch) {
                const parsed = JSON.parse(arrayMatch[0]);
                if (Array.isArray(parsed)) {
                    this.logger.info(`Parsed ${parsed.length} tool calls from AI response (array format)`);
                    return parsed;
                }
            }
        } catch (error) {
            this.logger.warn(`Failed to parse tool plan JSON from AI response: ${(error as Error).message}`);
        }

        this.logger.warn('No valid tool plan found in AI response');
        return [];
    }

    /**
     * ğŸš€ RAGçŸ¥è¯†æ£€ç´¢ï¼šåŸºäºç”¨æˆ·è¾“å…¥å’Œä¸Šä¸‹æ–‡æ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼ˆå¢å¼ºç‰ˆï¼šé›†æˆé¢„å¤„ç†åˆ†æï¼‰
     */
    private async retrieveRelevantKnowledge(userInput: string, sessionContext: SessionContext): Promise<string | null> {
        try {
            // ğŸš€ é›¶æˆæœ¬é¢„å¤„ç†ï¼šåœ¨è°ƒç”¨æ˜‚è´µçš„LLMä¹‹å‰è¿›è¡Œæœ¬åœ°åˆ†æ
            const preAnalysis = InputAnalyzer.analyzeInput(userInput);
            
            // è¯†åˆ«ç”¨æˆ·è¾“å…¥çš„å…³é”®è¯å’Œæ„å›¾
            const keywords = this.extractKeywords(userInput);
            const intent = this.identifyIntent(userInput);
            
            // æ„å»ºçŸ¥è¯†æ£€ç´¢ç»“æœï¼ˆå¢å¼ºç‰ˆï¼šä½¿ç”¨é¢„å¤„ç†åˆ†æï¼‰
            const knowledgeFragments: string[] = [];
            
            // ğŸš€ é¢„å¤„ç†åˆ†ææ‘˜è¦
            const analysisSummary = InputAnalyzer.generateAnalysisSummary(preAnalysis);
            knowledgeFragments.push(analysisSummary);
            
            // 1. åŸºäºé¢†åŸŸçš„æœ€ä½³å®è·µï¼ˆæ¥è‡ªé¢„å¤„ç†åˆ†æï¼‰
            if (preAnalysis.domain.domain !== 'general') {
                knowledgeFragments.push(`
ğŸ“‹ **${preAnalysis.domain.domain}é¢†åŸŸæœ€ä½³å®è·µ**ï¼š
- é¡¹ç›®ç±»å‹ï¼š${preAnalysis.projectType.type}
- åŒ¹é…å…³é”®è¯ï¼š${preAnalysis.domain.matchedKeywords.join(', ')}
- å»ºè®®å‚è€ƒè¡Œä¸šæ ‡å‡†çš„${preAnalysis.domain.domain}ç³»ç»Ÿè®¾è®¡æ¨¡å¼`);
            }
            
            // 2. åŸºäºæ„å›¾çš„æœ€ä½³å®è·µ
            if (intent === 'create_requirement') {
                knowledgeFragments.push(`
ğŸ“ **éœ€æ±‚åˆ›å»ºæœ€ä½³å®è·µ**ï¼š
- æ¯ä¸ªéœ€æ±‚åº”å…·æœ‰å”¯ä¸€æ ‡è¯†ç¬¦å’Œæ¸…æ™°æè¿°
- éœ€æ±‚åº”åŒ…å«éªŒæ”¶æ ‡å‡†å’Œä¼˜å…ˆçº§
- å»ºè®®å…ˆæ£€æŸ¥ç°æœ‰éœ€æ±‚ï¼Œé¿å…é‡å¤åˆ›å»º
- åŠŸèƒ½éœ€æ±‚åº”å…³è”åˆ°å…·ä½“çš„ç”¨æˆ·æ•…äº‹`);
            } else if (intent === 'edit_requirement') {
                knowledgeFragments.push(`
âœï¸ **éœ€æ±‚ç¼–è¾‘æœ€ä½³å®è·µ**ï¼š
- ç¼–è¾‘å‰åº”å…ˆè·å–å½“å‰éœ€æ±‚è¯¦æƒ…
- é‡å¤§ä¿®æ”¹åº”è®°å½•å˜æ›´å†å²
- ç¡®ä¿ä¿®æ”¹åçš„éœ€æ±‚ä¸å…¶ä»–éœ€æ±‚ä¿æŒä¸€è‡´æ€§
- ä¿®æ”¹ååº”éªŒè¯éœ€æ±‚çš„å®Œæ•´æ€§`);
            } else if (intent === 'manage_project') {
                knowledgeFragments.push(`
ğŸ—ï¸ **é¡¹ç›®ç®¡ç†æœ€ä½³å®è·µ**ï¼š
- é¡¹ç›®åˆå§‹åŒ–åº”åŒ…å«åŸºæœ¬çš„ç›®å½•ç»“æ„
- é‡è¦æ–‡ä»¶åº”ä½¿ç”¨æ¨¡æ¿ç¡®ä¿ä¸€è‡´æ€§
- å®šæœŸå¤‡ä»½é‡è¦çš„é¡¹ç›®æ–‡ä»¶
- ä¿æŒé¡¹ç›®æ–‡æ¡£çš„æ›´æ–°å’ŒåŒæ­¥`);
            }
            
            // 2. åŸºäºå…³é”®è¯çš„æŠ€æœ¯çŸ¥è¯†
            if (keywords.some(k => ['ç”¨æˆ·', 'ç™»å½•', 'è®¤è¯', 'æˆæƒ'].includes(k))) {
                knowledgeFragments.push(`
ğŸ” **ç”¨æˆ·è®¤è¯ç›¸å…³çŸ¥è¯†**ï¼š
- ç”¨æˆ·è®¤è¯é€šå¸¸åŒ…æ‹¬ç”¨æˆ·å/å¯†ç ã€å¤šå› ç´ è®¤è¯
- éœ€è¦è€ƒè™‘å¯†ç å®‰å…¨ç­–ç•¥å’Œä¼šè¯ç®¡ç†
- åº”å®šä¹‰ç”¨æˆ·æƒé™å’Œè§’è‰²ç®¡ç†æœºåˆ¶`);
            }
            
            // 3. å½“å‰é¡¹ç›®ä¸Šä¸‹æ–‡ç›¸å…³çŸ¥è¯†
            if (sessionContext.projectName) {
                knowledgeFragments.push(`
ğŸ“Š **å½“å‰é¡¹ç›®ä¸Šä¸‹æ–‡**ï¼š
- é¡¹ç›®ï¼š${sessionContext.projectName}
- æ´»è·ƒæ–‡ä»¶ï¼š${sessionContext.activeFiles?.length || 0}ä¸ª
- å»ºè®®åœ¨æ“ä½œå‰å…ˆäº†è§£é¡¹ç›®å½“å‰çŠ¶æ€`);
            }
            
            return knowledgeFragments.length > 0 ? knowledgeFragments.join('\n\n') : null;
            
        } catch (error) {
            this.logger.warn(`Failed to retrieve relevant knowledge: ${(error as Error).message}`);
            return null;
        }
    }

    /**
     * ä»ç”¨æˆ·è¾“å…¥ä¸­æå–å…³é”®è¯
     */
    private extractKeywords(userInput: string): string[] {
        const keywords = userInput.toLowerCase()
            .split(/[\s,ï¼Œã€‚ï¼ï¼Ÿ;ï¼›ï¼š]+/)
            .filter(word => word.length > 1);
        return keywords;
    }

    /**
     * è¯†åˆ«ç”¨æˆ·æ„å›¾
     */
    private identifyIntent(userInput: string): string {
        const input = userInput.toLowerCase();
        
        if (input.includes('åˆ›å»º') || input.includes('æ–°å¢') || input.includes('æ·»åŠ ')) {
            return 'create_requirement';
        } else if (input.includes('ç¼–è¾‘') || input.includes('ä¿®æ”¹') || input.includes('æ›´æ–°')) {
            return 'edit_requirement';
        } else if (input.includes('é¡¹ç›®') || input.includes('åˆå§‹åŒ–') || input.includes('åˆ›å»ºé¡¹ç›®')) {
            return 'manage_project';
        } else if (input.includes('æŸ¥çœ‹') || input.includes('æ˜¾ç¤º') || input.includes('åˆ—å‡º')) {
            return 'view_information';
        } else if (input.includes('åˆ é™¤') || input.includes('ç§»é™¤')) {
            return 'delete_item';
        }
        
        return 'general_query';
    }

    /**
     * æ‰§è¡Œå·¥å…·è°ƒç”¨ - ä½¿ç”¨æ–°çš„å·¥å…·æ‰§è¡Œå™¨
     */
    private async executeToolCalls(toolCalls: Array<{ name: string; args: any }>): Promise<any[]> {
        const results = [];

        for (const toolCall of toolCalls) {
            try {
                this.logger.info(`ğŸ”§ Executing tool: ${toolCall.name}`);
                
                const result = await toolExecutor.executeTool(
                    toolCall.name,
                    toolCall.args
                );
                
                results.push({
                    success: result.success,
                    result: result.result,
                    toolName: toolCall.name
                });
                
            } catch (error) {
                this.logger.error(`âŒ Tool execution failed: ${toolCall.name}`);
                results.push({
                    success: false,
                    error: (error as Error).message,
                    toolName: toolCall.name
                });
            }
        }

        return results;
    }

    /**
     * ğŸš€ æ±‡æ€»å·¥å…·æ‰§è¡Œç»“æœ
     */
    private summarizeToolResults(results: any[]): string {
        const successful = results.filter(r => r.success);
        const failed = results.filter(r => !r.success);
        
        if (failed.length === 0) {
            return `âœ… æˆåŠŸæ‰§è¡Œäº† ${successful.length} ä¸ªæ“ä½œ`;
        } else if (successful.length === 0) {
            return `âŒ ${failed.length} ä¸ªæ“ä½œæ‰§è¡Œå¤±è´¥`;
        } else {
            return `âš ï¸ ${successful.length} ä¸ªæ“ä½œæˆåŠŸï¼Œ${failed.length} ä¸ªæ“ä½œå¤±è´¥`;
        }
    }

    /**
     * ç”ŸæˆæŒ‡å¯¼æ€§å“åº”ï¼ˆå½“æ— æ³•ç”Ÿæˆå·¥å…·è®¡åˆ’æ—¶ï¼‰
     */
    private generateGuidanceResponse(userInput: string, sessionContext: SessionContext): string {
        const availableTools = getAvailableTools().map(t => t.name);
        const hasProject = !!sessionContext.projectName;
        
        if (!hasProject) {
            return `ç›®å‰æ²¡æœ‰æ´»è·ƒçš„é¡¹ç›®ã€‚å¯ç”¨çš„æ“ä½œåŒ…æ‹¬ï¼š${availableTools.join(', ')}ã€‚è¯·å…ˆåˆ›å»ºä¸€ä¸ªé¡¹ç›®æˆ–æ‰“å¼€ç°æœ‰é¡¹ç›®ã€‚`;
        }
        
        return `æˆ‘ç†è§£æ‚¨çš„è¯·æ±‚æ˜¯ï¼š"${userInput}"ã€‚å½“å‰å¯ç”¨çš„å·¥å…·æœ‰ï¼š${availableTools.join(', ')}ã€‚è¯·å°è¯•æ›´å…·ä½“åœ°æè¿°æ‚¨æƒ³è¦æ‰§è¡Œçš„æ“ä½œã€‚`;
    }

    /**
     * æ£€æŸ¥æ¨¡å‹ä½¿ç”¨æƒé™
     */
    private async checkModelPermissions(model: vscode.LanguageModelChat): Promise<boolean> {
        try {
            return model && typeof model.sendRequest === 'function';
        } catch (error) {
            this.logger.warn('Failed to check model permissions');
            return false;
        }
    }

    /**
     * åŠ è½½é…ç½®
     */
    private loadConfiguration(): void {
        try {
            const config = vscode.workspace.getConfiguration('srs-writer');
            this.useAIOrchestrator = config.get('useAIOrchestrator', true);
            
            this.logger.info(`Orchestrator configuration loaded: AI=${this.useAIOrchestrator}`);
        } catch (error) {
            this.logger.warn(`Failed to load configuration: ${(error as Error).message}`);
        }
    }

    /**
     * åŠ¨æ€åˆ‡æ¢AIæ¨¡å¼
     */
    public setAIMode(enabled: boolean): void {
        this.useAIOrchestrator = enabled;
        this.logger.info(`Orchestrator AI mode ${enabled ? 'enabled' : 'disabled'}`);
    }

    /**
     * è·å–å½“å‰çŠ¶æ€
     */
    public getStatus(): { 
        aiMode: boolean; 
        toolMode: boolean;
        availableTools: string[];
    } {
        return {
            aiMode: this.useAIOrchestrator,
            toolMode: true, // v2.0 always in tool mode
            availableTools: getAvailableTools().map(t => t.name)
        };
    }



    /**
     * è·å–ç³»ç»ŸçŠ¶æ€
     */
    async getSystemStatus(): Promise<any> {
        const toolStats = toolExecutor.getExecutionStats();

        return {
            mode: 'Chain-of-Thought AI Tool Agent',
            version: '2.1',
            status: 'Active',
            toolSystem: toolStats,
            capabilities: [
                'ğŸ§  æ€ç»´é“¾æ¨ç† (Chain-of-Thought)',
                'ğŸ”„ å¯¹è¯å¼è§„åˆ’å¾ªç¯',
                'ğŸš€ è‡ªæˆ‘ä¿®æ­£ä¸é€‚åº”',
                'ğŸ“š RAGçŸ¥è¯†æ£€ç´¢å¢å¼º',
                'ğŸ› ï¸ æ™ºèƒ½å·¥å…·åè°ƒæ‰§è¡Œ',
                'ğŸ“ æ™ºèƒ½éœ€æ±‚ç®¡ç†',
                'ğŸ“„ æ–‡æ¡£è‡ªåŠ¨åŒ–æ“ä½œ',
                'ğŸ”§ ç¼–è¾‘å™¨æ·±åº¦é›†æˆ'
            ],
            features: {
                chainOfThought: true,
                conversationalPlanning: true,
                selfCorrection: true,
                ragKnowledgeRetrieval: true,
                adaptiveToolExecution: true
            }
        };
    }

    /**
     * é‡ç½®ç³»ç»ŸçŠ¶æ€
     */
    async resetSystem(): Promise<void> {
        toolExecutor.resetStats();
        this.logger.info('ğŸ”„ System reset completed');
    }

    // åŠ¨æ€æ¨¡å‹é…ç½®ç¼“å­˜
    private static modelConfigCache = new Map<string, {
        maxTokens: number;
        warningThreshold: number;
        compressionThreshold: number;
        lastUpdated: number;
        confidence: 'low' | 'medium' | 'high';
    }>();

    /**
     * ğŸš€ åŠ¨æ€ä¸Šä¸‹æ–‡çª—å£é…ç½®ï¼šå¤šå±‚æ¬¡è‡ªé€‚åº”ç­–ç•¥
     */
    private async getContextWindowConfig(selectedModel: vscode.LanguageModelChat): Promise<{
        maxTokens: number;
        warningThreshold: number;
        compressionThreshold: number;
    }> {
        const modelKey = selectedModel.name;
        
        // 1. ä¼˜å…ˆçº§1ï¼šç”¨æˆ·é…ç½®è¦†ç›–
        const userConfig = await this.loadUserModelConfig(modelKey);
        if (userConfig) {
            const config = {
                maxTokens: userConfig.maxTokens || 8000,
                warningThreshold: userConfig.warningThreshold || Math.floor((userConfig.maxTokens || 8000) * 0.75),
                compressionThreshold: userConfig.compressionThreshold || Math.floor((userConfig.maxTokens || 8000) * 0.6)
            };
            this.logger.info(`ğŸ‘¤ Using user config for ${modelKey}: ${config.maxTokens} tokens`);
            return config;
        }
        
        // 2. ä¼˜å…ˆçº§2ï¼šé”™è¯¯å­¦ä¹ ç¼“å­˜ï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰
        const cached = Orchestrator.modelConfigCache.get(modelKey);
        if (cached && cached.confidence === 'high') {
            this.logger.info(`ğŸ¯ Using learned config for ${modelKey}: ${cached.maxTokens} tokens (high confidence)`);
            return {
                maxTokens: cached.maxTokens,
                warningThreshold: cached.warningThreshold,
                compressionThreshold: cached.compressionThreshold
            };
        }
        
        // 3. ä¼˜å…ˆçº§3ï¼šæ™®é€šç¼“å­˜ï¼ˆ24å°æ—¶æœ‰æ•ˆï¼‰
        if (cached && (Date.now() - cached.lastUpdated) < 24 * 60 * 60 * 1000) {
            this.logger.info(`ğŸ“‹ Using cached config for ${modelKey} (confidence: ${cached.confidence})`);
            return {
                maxTokens: cached.maxTokens,
                warningThreshold: cached.warningThreshold,
                compressionThreshold: cached.compressionThreshold
            };
        }
        
        // 4. ä¼˜å…ˆçº§4ï¼šå¯å‘å¼æ¨æ–­ï¼ˆåŸºäºé€šç”¨è§„åˆ™ï¼‰
        const inferredConfig = this.inferModelCapabilities(selectedModel);
        
        // 5. ç¼“å­˜æ¨æ–­ç»“æœ
        Orchestrator.modelConfigCache.set(modelKey, {
            ...inferredConfig,
            lastUpdated: Date.now(),
            confidence: 'medium'
        });
        
        this.logger.info(`ğŸ” Inferred config for ${modelKey}: ${inferredConfig.maxTokens} tokens (medium confidence)`);
        return {
            maxTokens: inferredConfig.maxTokens,
            warningThreshold: inferredConfig.warningThreshold,
            compressionThreshold: inferredConfig.compressionThreshold
        };
    }

    /**
     * ğŸš€ å¯å‘å¼æ¨¡å‹èƒ½åŠ›æ¨æ–­ï¼šåŸºäºåç§°æ¨¡å¼å’Œé€šç”¨è§„åˆ™
     */
    private inferModelCapabilities(model: vscode.LanguageModelChat) {
        const modelName = model.name.toLowerCase();
        
        // åŸºäºå¹´ä»½å’Œç‰ˆæœ¬çš„å¯å‘å¼æ¨æ–­
        const currentYear = new Date().getFullYear();
        
        // æ£€æŸ¥æ˜¯å¦æ˜¯æ–°ä¸€ä»£å¤§ä¸Šä¸‹æ–‡æ¨¡å‹
        if (this.isLargeContextModel(modelName)) {
            return { maxTokens: 128000, warningThreshold: 100000, compressionThreshold: 80000 };
        }
        
        // æ£€æŸ¥æ˜¯å¦æ˜¯ä¸­ç­‰ä¸Šä¸‹æ–‡æ¨¡å‹  
        if (this.isMediumContextModel(modelName)) {
            return { maxTokens: 32000, warningThreshold: 25000, compressionThreshold: 20000 };
        }
        
        // æ£€æŸ¥æ˜¯å¦æ˜¯è€æ¨¡å‹æˆ–å°æ¨¡å‹
        if (this.isSmallContextModel(modelName)) {
            return { maxTokens: 4000, warningThreshold: 3000, compressionThreshold: 2000 };
        }
        
        // é»˜è®¤ä¿å®ˆä¼°è®¡
        return { maxTokens: 8000, warningThreshold: 6000, compressionThreshold: 4000 };
    }

    /**
     * æ£€æŸ¥æ˜¯å¦æ˜¯å¤§ä¸Šä¸‹æ–‡æ¨¡å‹
     */
    private isLargeContextModel(modelName: string): boolean {
        const largeContextIndicators = [
            'turbo', '128k', '200k', 'long', 'extended',
            'claude-3', 'claude-2.1', 'gemini-pro',
            '2024', '2023'  // è¾ƒæ–°çš„æ¨¡å‹é€šå¸¸æœ‰æ›´å¤§çš„ä¸Šä¸‹æ–‡
        ];
        return largeContextIndicators.some(indicator => modelName.includes(indicator));
    }

    /**
     * æ£€æŸ¥æ˜¯å¦æ˜¯ä¸­ç­‰ä¸Šä¸‹æ–‡æ¨¡å‹
     */
    private isMediumContextModel(modelName: string): boolean {
        const mediumContextIndicators = [
            'gpt-4', 'claude-2', 'gemini',
            '16k', '32k'
        ];
        return mediumContextIndicators.some(indicator => modelName.includes(indicator));
    }

    /**
     * æ£€æŸ¥æ˜¯å¦æ˜¯å°ä¸Šä¸‹æ–‡æ¨¡å‹
     */
    private isSmallContextModel(modelName: string): boolean {
        const smallContextIndicators = [
            'gpt-3.5', '4k', '2k', 
            '2022', '2021'  // è¾ƒè€çš„æ¨¡å‹
        ];
        return smallContextIndicators.some(indicator => modelName.includes(indicator));
    }

    /**
     * ğŸš€ Tokenæ•°é‡ä¼°ç®—ï¼šç®€å•ä½†æœ‰æ•ˆçš„ä¼°ç®—æ–¹æ³•
     */
    private estimateTokens(text: string): number {
        // ç®€å•ä¼°ç®—ï¼š1 token â‰ˆ 0.75 è‹±æ–‡å•è¯ â‰ˆ 4 å­—ç¬¦
        // å¯¹äºä¸­æ–‡ï¼Œ1ä¸ªå­—ç¬¦å¤§çº¦ç­‰äº1ä¸ªtoken
        const chineseChars = (text.match(/[\u4e00-\u9fff]/g) || []).length;
        const englishWords = text.replace(/[\u4e00-\u9fff]/g, '').split(/\s+/).filter(w => w.length > 0).length;
        
        return Math.ceil(chineseChars + englishWords * 1.3);
    }

    /**
     * ğŸš€ å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†ï¼šæ™ºèƒ½å‹ç¼©å†å²è®°å½• + é”™è¯¯åé¦ˆå­¦ä¹ 
     */
    private async manageConversationContext(
        conversationHistory: Array<{ role: string; content: string; tokens?: number; toolResults?: any[] }>,
        contextConfig: { maxTokens: number; warningThreshold: number; compressionThreshold: number },
        selectedModel: vscode.LanguageModelChat
    ): Promise<void> {
        const totalTokens = conversationHistory.reduce((sum, item) => sum + (item.tokens || 0), 0);
        
        if (totalTokens <= contextConfig.compressionThreshold) {
            return; // æ— éœ€å‹ç¼©
        }
        
        this.logger.warn(`ğŸ’­ Context approaching limit (${totalTokens}/${contextConfig.maxTokens} tokens), compressing history`);
        
        // ä¿ç•™æœ€æ–°çš„2è½®å¯¹è¯å’Œåˆå§‹ç”¨æˆ·è¾“å…¥
        const originalUserInput = conversationHistory[0];
        const recentHistory = conversationHistory.slice(-4); // æœ€è¿‘2è½®å¯¹è¯
        
        // å‹ç¼©ä¸­é—´å†å²ä¸ºæ‘˜è¦
        const middleHistory = conversationHistory.slice(1, -4);
        if (middleHistory.length > 0) {
            const compressionSummary = await this.compressHistoryToSummary(middleHistory, selectedModel);
            
            // é‡æ„å¯¹è¯å†å²
            conversationHistory.length = 0; // æ¸…ç©ºæ•°ç»„
            conversationHistory.push(originalUserInput);
            
            if (compressionSummary) {
                conversationHistory.push({
                    role: 'system',
                    content: `ğŸ“‹ **å†å²æ‘˜è¦**: ${compressionSummary}`,
                    tokens: this.estimateTokens(compressionSummary)
                });
            }
            
            conversationHistory.push(...recentHistory);
            
            const newTotalTokens = conversationHistory.reduce((sum, item) => sum + (item.tokens || 0), 0);
            this.logger.info(`âœ… Context compressed: ${totalTokens} â†’ ${newTotalTokens} tokens`);
        }
    }

    /**
     * ğŸš€ é”™è¯¯åé¦ˆå­¦ä¹ ï¼šä»å®é™…APIé”™è¯¯ä¸­å­¦ä¹ æ¨¡å‹é™åˆ¶
     */
    private async handleContextError(
        error: Error,
        selectedModel: vscode.LanguageModelChat,
        estimatedTokens: number
    ): Promise<void> {
        const errorMessage = error.message.toLowerCase();
        
        // æ£€æŸ¥æ˜¯å¦æ˜¯ä¸Šä¸‹æ–‡è¶…é™é”™è¯¯
        if (this.isContextLimitError(errorMessage)) {
            const modelKey = selectedModel.name;
            const cached = Orchestrator.modelConfigCache.get(modelKey);
            
            if (cached) {
                // æ ¹æ®é”™è¯¯è°ƒæ•´é…ç½®ï¼ˆä¿å®ˆé™ä½ï¼‰
                const newMaxTokens = Math.floor(estimatedTokens * 0.8); // é™ä½20%
                const updatedConfig = {
                    maxTokens: newMaxTokens,
                    warningThreshold: Math.floor(newMaxTokens * 0.8),
                    compressionThreshold: Math.floor(newMaxTokens * 0.6),
                    lastUpdated: Date.now(),
                    confidence: 'high' as const // ä»å®é™…é”™è¯¯å­¦ä¹ ï¼Œç½®ä¿¡åº¦æœ€é«˜
                };
                
                Orchestrator.modelConfigCache.set(modelKey, updatedConfig);
                
                this.logger.warn(`ğŸ”§ Learned from context error for ${modelKey}: ${cached.maxTokens} â†’ ${newMaxTokens} tokens`);
            }
        }
    }

    /**
     * æ£€æŸ¥é”™è¯¯æ˜¯å¦ä¸ºä¸Šä¸‹æ–‡é™åˆ¶é”™è¯¯
     */
    private isContextLimitError(errorMessage: string): boolean {
        const contextErrorIndicators = [
            'context length',
            'token limit',
            'maximum context',
            'too long',
            'context size',
            '4096', '8192', '16384', '32768'
        ];
        return contextErrorIndicators.some(indicator => errorMessage.includes(indicator));
    }

    /**
     * ğŸš€ ç”¨æˆ·é…ç½®è¦†ç›–ï¼šå…è®¸ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹é…ç½®
     */
    private async loadUserModelConfig(modelName: string): Promise<{
        maxTokens?: number;
        warningThreshold?: number;
        compressionThreshold?: number;
    } | null> {
        try {
            const config = vscode.workspace.getConfiguration('srs-writer');
            const userModelConfigs = config.get<Record<string, any>>('modelConfigs', {});
            
            const userConfig = userModelConfigs[modelName];
            if (userConfig && typeof userConfig === 'object') {
                this.logger.info(`ğŸ‘¤ Using user-defined config for ${modelName}`);
                return userConfig;
            }
        } catch (error) {
            this.logger.warn(`Failed to load user model config: ${(error as Error).message}`);
        }
        return null;
    }

    /**
     * ğŸš€ å†å²å‹ç¼©ï¼šå°†å¤šè½®å¯¹è¯å‹ç¼©ä¸ºç®€æ´æ‘˜è¦
     */
    private async compressHistoryToSummary(
        historyToCompress: Array<{ role: string; content: string; toolResults?: any[] }>,
        selectedModel: vscode.LanguageModelChat
    ): Promise<string | null> {
        try {
            // æ„å»ºå‹ç¼©æç¤ºè¯
            const historyText = historyToCompress.map((item, index) => {
                if (item.toolResults) {
                    const successCount = item.toolResults.filter(r => r.success).length;
                    const toolNames = item.toolResults.map(r => r.toolName).join(', ');
                    return `ç¬¬${index + 1}æ­¥: æ‰§è¡Œå·¥å…·[${toolNames}] - ${successCount}/${item.toolResults.length}æˆåŠŸ`;
                }
                return `${item.role}: ${item.content.substring(0, 100)}...`;
            }).join('\n');

            const compressionPrompt = `è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²å‹ç¼©ä¸º1-2å¥è¯çš„ç®€æ´æ‘˜è¦ï¼Œé‡ç‚¹å…³æ³¨ï¼š
1. å®Œæˆäº†å“ªäº›å…³é”®æ“ä½œ
2. æ˜¯å¦æœ‰å¤±è´¥éœ€è¦ä¿®æ­£
3. å½“å‰è¿›å±•çŠ¶æ€

å†å²è®°å½•ï¼š
${historyText}

æ‘˜è¦ï¼ˆ1-2å¥è¯ï¼‰ï¼š`;

            const messages = [vscode.LanguageModelChatMessage.User(compressionPrompt)];
            const response = await selectedModel.sendRequest(messages, {
                justification: 'Compress conversation history for context management'
            });

            let summary = '';
            for await (const fragment of response.text) {
                summary += fragment;
            }

            return summary.trim();
        } catch (error) {
            this.logger.warn(`Failed to compress history: ${(error as Error).message}`);
            return null;
        }
    }

    /**
     * ğŸš€ å·¥å…·ç»“æœæ ¼å¼åŒ–ï¼šç”Ÿæˆç»“æ„åŒ–çš„æ‰§è¡ŒæŠ¥å‘Š
     */
    private formatToolResults(toolResults: any[]): string {
        const successfulTools = toolResults.filter(r => r.success);
        const failedTools = toolResults.filter(r => !r.success);
        
        let report = `ğŸ”§ **å·¥å…·æ‰§è¡ŒæŠ¥å‘Š** (${successfulTools.length}/${toolResults.length}æˆåŠŸ)\n\n`;
        
        if (successfulTools.length > 0) {
            report += `âœ… **æˆåŠŸæ‰§è¡Œ**:\n`;
            successfulTools.forEach(tool => {
                report += `  â€¢ ${tool.toolName}: æ‰§è¡ŒæˆåŠŸ\n`;
            });
            report += '\n';
        }
        
        if (failedTools.length > 0) {
            report += `âŒ **æ‰§è¡Œå¤±è´¥**:\n`;
            failedTools.forEach(tool => {
                report += `  â€¢ ${tool.toolName}: ${tool.error || 'æœªçŸ¥é”™è¯¯'}\n`;
            });
            report += '\n';
        }
        
        return report;
    }

    /**
     * æ¸…ç†èµ„æº
     */
    public dispose(): void {
        this.logger.info('Orchestrator disposed');
    }
}
